(self.webpackChunkhpcdocs=self.webpackChunkhpcdocs||[]).push([[494],{4155:function(e,n,t){"use strict";t.r(n),t.d(n,{frontMatter:function(){return i},metadata:function(){return a},toc:function(){return l},default:function(){return c}});var o=t(2122),s=t(9756),r=(t(7294),t(3905)),i={slug:"slurm"},a={unversionedId:"snippets/slurm",id:"snippets/slurm",isDocsHomePage:!1,title:"Slurm Snippets",description:"Interactive Sessions",source:"@site/docs/snippets/01_slurm.md",sourceDirName:"snippets",slug:"/snippets/slurm",permalink:"/docs/snippets/slurm",editUrl:"https://github.com/irhpc/irhpc.github.io/edit/main/docs/snippets/01_slurm.md",version:"current",sidebarPosition:1,frontMatter:{slug:"slurm"},sidebar:"tutorialSidebar",previous:{title:"Hosting",permalink:"/docs/documentation/hosting"}},l=[{value:"Interactive Sessions",id:"interactive-sessions",children:[{value:"Move to the Node",id:"move-to-the-node",children:[]},{value:"Request Nodes",id:"request-nodes",children:[]}]}],u={toc:l};function c(e){var n=e.components,t=(0,s.Z)(e,["components"]);return(0,r.kt)("wrapper",(0,o.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("h2",{id:"interactive-sessions"},"Interactive Sessions"),(0,r.kt)("h3",{id:"move-to-the-node"},"Move to the Node"),(0,r.kt)("p",null,"We can request a job with two compute nodes, and 48 tasks over for thirty minutes."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"srun -N 2 -n 48 -t 30 --pty /bin/bash\n# This brings you to the node directly\n")),(0,r.kt)("p",null,"Recall that we have the ",(0,r.kt)("inlineCode",{parentName:"p"},"help")," details, along with the manpages ",(0,r.kt)("inlineCode",{parentName:"p"},"man srun")," which ",(0,r.kt)("a",{parentName:"p",href:"http://manpages.org/srun/1"},"may be accessed here"),"."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},'\u276f srun --help\nUsage: srun [OPTIONS(0)... [executable(0) [args(0)...]]] [ : [OPTIONS(N)...]] executable(N) [args(N)...]\n\nParallel run options:\n  -A, --account=name          charge job to specified account\n      --acctg-freq=<datatype>=<interval> accounting and profiling sampling\n                              intervals. Supported datatypes:\n                              task=<interval> energy=<interval>\n                              network=<interval> filesystem=<interval>\n      --bb=<spec>             burst buffer specifications\n      --bbf=<file_name>       burst buffer specification file\n      --bcast=<dest_path>     Copy executable file to compute nodes\n  -b, --begin=time            defer job until HH:MM MM/DD/YY\n  -c, --cpus-per-task=ncpus   number of cpus required per task\n      --comment=name          arbitrary comment\n      --compress[=library]    data compression library used with --bcast\n      --cpu-freq=min[-max[:gov]] requested cpu frequency (and governor)\n  -d, --dependency=type:jobid[:time] defer job until condition on jobid is satisfied\n      --deadline=time         remove the job if no ending possible before\n                              this deadline (start > (deadline - time[-min]))\n      --delay-boot=mins       delay boot for desired node features\n  -D, --chdir=path            change remote current working directory\n      --export=env_vars|NONE  environment variables passed to launcher with\n                              optional values or NONE (pass no variables)\n  -e, --error=err             location of stderr redirection\n      --epilog=program        run "program" after launching job step\n  -E, --preserve-env          env vars for node and task counts override\n                              command-line flags\n      --gres=list             required generic resources\n      --gres-flags=opts       flags related to GRES management\n  -H, --hold                  submit job in held state\n  -i, --input=in              location of stdin redirection\n  -I, --immediate[=secs]      exit if resources not available in "secs"\n      --jobid=id              run under already allocated job\n  -J, --job-name=jobname      name of job\n  -k, --no-kill               do not kill job on node failure\n  -K, --kill-on-bad-exit      kill the job if any task terminates with a\n                              non-zero exit code\n  -l, --label                 prepend task number to lines of stdout/err\n  -L, --licenses=names        required license, comma separated\n  -M, --clusters=names        Comma separated list of clusters to issue\n                              commands to.  Default is current cluster.\n                              Name of \'all\' will submit to run on all clusters.\n                              NOTE: SlurmDBD must up.\n  -m, --distribution=type     distribution method for processes to nodes\n                              (type = block|cyclic|arbitrary)\n      --mail-type=type        notify on state change: BEGIN, END, FAIL or ALL\n      --mail-user=user        who to send email notification for job state\n                              changes\n      --mcs-label=mcs         mcs label if mcs plugin mcs/group is used\n      --mpi=type              type of MPI being used\n      --multi-prog            if set the program name specified is the\n                              configuration specification for multiple programs\n  -n, --ntasks=ntasks         number of tasks to run\n      --nice[=value]          decrease scheduling priority by value\n      --ntasks-per-node=n     number of tasks to invoke on each node\n  -N, --nodes=N               number of nodes on which to run (N = min[-max])\n  -o, --output=out            location of stdout redirection\n  -O, --overcommit            overcommit resources\n      --het-group=value       hetjob component allocation(s) in which to launch\n                              application\n  -p, --partition=partition   partition requested\n      --power=flags           power management options\n      --priority=value        set the priority of the job to value\n      --prolog=program        run "program" before launching job step\n      --profile=value         enable acct_gather_profile for detailed data\n                              value is all or none or any combination of\n                              energy, lustre, network or task\n      --propagate[=rlimits]   propagate all [or specific list of] rlimits\n      --pty                   run task zero in pseudo terminal\n      --quit-on-interrupt     quit on single Ctrl-C\n  -q, --qos=qos               quality of service\n  -Q, --quiet                 quiet mode (suppress informational messages)\n      --reboot                reboot block before starting job\n  -r, --relative=n            run job step relative to node n of allocation\n  -s, --oversubscribe         over-subscribe resources with other jobs\n  -S, --core-spec=cores       count of reserved cores\n      --signal=[R:]num[@time] send signal when time limit within time seconds\n      --slurmd-debug=level    slurmd debug level\n      --spread-job            spread job across as many nodes as possible\n      --switches=max-switches{@max-time-to-wait}\n                              Optimum switches and max time to wait for optimum\n      --task-epilog=program   run "program" after launching task\n      --task-prolog=program   run "program" before launching task\n      --thread-spec=threads   count of reserved threads\n  -T, --threads=threads       set srun launch fanout\n  -t, --time=minutes          time limit\n      --time-min=minutes      minimum time limit (if distinct)\n  -u, --unbuffered            do not line-buffer stdout/err\n      --use-min-nodes         if a range of node counts is given, prefer the\n                              smaller count\n  -v, --verbose               verbose mode (multiple -v\'s increase verbosity)\n  -W, --wait=sec              seconds to wait after first task exits\n                              before killing job\n      --wckey=wckey           wckey to run job under\n  -X, --disable-status        Disable Ctrl-C status feature\n\nConstraint options:\n      --cluster-constraint=list specify a list of cluster-constraints\n      --contiguous            demand a contiguous range of nodes\n  -C, --constraint=list       specify a list of constraints\n      --mem=MB                minimum amount of real memory\n      --mincpus=n             minimum number of logical processors (threads)\n                              per node\n      --reservation=name      allocate resources from named reservation\n      --tmp=MB                minimum amount of temporary disk\n  -w, --nodelist=hosts...     request a specific list of hosts\n  -x, --exclude=hosts...      exclude a specific list of hosts\n  -Z, --no-allocate           don\'t allocate nodes (must supply -w)\n\nConsumable resources related options:\n      --exclusive[=user]      allocate nodes in exclusive mode when\n                              cpu consumable resource is enabled\n                              or don\'t share CPUs for job steps\n      --exclusive[=mcs]       allocate nodes in exclusive mode when\n                              cpu consumable resource is enabled\n                              and mcs plugin is enabled\n                              or don\'t share CPUs for job steps\n      --mem-per-cpu=MB        maximum amount of real memory per allocated\n                              cpu required by the job.\n                              --mem >= --mem-per-cpu if --mem is specified.\n      --resv-ports            reserve communication ports\n\nAffinity/Multi-core options: (when the task/affinity plugin is enabled)\n  -B, --extra-node-info=S[:C[:T]]           Expands to:\n      --sockets-per-node=S    number of sockets per node to allocate\n      --cores-per-socket=C    number of cores per socket to allocate\n      --threads-per-core=T    number of threads per core to allocate\n                              each field can be \'min\' or wildcard \'*\'\n                              total cpus requested = (N x S x C x T)\n\n      --ntasks-per-core=n     number of tasks to invoke on each core\n      --ntasks-per-socket=n   number of tasks to invoke on each socket\n      --cpu-bind=             Bind tasks to CPUs\n                              (see "--cpu-bind=help" for options)\n      --hint=                 Bind tasks according to application hints\n                              (see "--hint=help" for options)\n      --mem-bind=             Bind memory to locality domains (ldom)\n                              (see "--mem-bind=help" for options)\n\nGPU scheduling options:\n      --cpus-per-gpu=n        number of CPUs required per allocated GPU\n  -G, --gpus=n                count of GPUs required for the job\n      --gpu-bind=...          task to gpu binding options\n      --gpu-freq=...          frequency and voltage of GPUs\n      --gpus-per-node=n       number of GPUs required per allocated node\n      --gpus-per-socket=n     number of GPUs required per allocated socket\n      --gpus-per-task=n       number of GPUs required per spawned task\n      --mem-per-gpu=n         real memory required per allocated GPU\n\nHelp options:\n  -h, --help                  show this help message\n      --usage                 display brief usage message\n\nOther options:\n  -V, --version               output version information and exit\n')),(0,r.kt)("h3",{id:"request-nodes"},"Request Nodes"),(0,r.kt)("p",null,"With ",(0,r.kt)("inlineCode",{parentName:"p"},"sallocate")," we do not get shifted to the node, this is useful for callbacks."))}c.isMDXComponent=!0}}]);